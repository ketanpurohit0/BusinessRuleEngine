Date:2020-08-19

PostgreSql
	1. Test with different col types
	
	-- drop table
	DROP TABLE IF EXISTS foo
	
	-- create table
	CREATE TABLE foo
	(
		name varchar(10) NOT NULL,
		age  int,
		dob  date,
		value decimal,
		seq serial,
		ts timestamp,
		js json,
		jsb jsonb,
		metadata varchar(5)[],
		moremetadata varchar(5)[3]	
	)
	-- get character encoding
	SHOW SERVER_ENCODING
	
	-- getting timestamps
	select current_timestamp,localtimestamp

	-- add some data
	INSERT INTO foo (name, age, dob, value, ts, js, jsb, metadata, moremetadata)
	VALUES
	('Name',32, '2020-03-20', 13.00, localtimestamp, '{ "foo" : "bar", "inner": {"more" : "foo"}}','{ "foo" : "bar", "inner": {"more" : "foo"}}', '{"a","b","c"}', '{"aa","bb"}')
	
	-- arrays all, first, first two, second and third, last
	SELECT metadata, metadata[1], metadata[1:2], metadata[2:3] , metadata[array_upper(metadata, 1)] FROM foo
	-- json
 	SELECT js->'inner'-> 'more' asJson, js->'inner'->>'more' asTxt FROM foo
	-- version
	SELECT version()


	
	2. Test via pyodbc and SqlAlchemy
	
	See PostgreSql.txt (SqlAlchemy returns json as dict, array as list. pyodbc does not)
	
	3. JDBC driver for PostgreSql
	Downloaded postgresql-42.2.14.jar
	
WebScrape

	1. Rename git repo
	

Apache Spark

	1. Set-Up cluster
	2. Try sql against PostgreSql
	conn = "jdbc:postgresql://localhost/postgres?user=postgres&password=*secret*"
	cd C:\MyInstalled\spark-2.4.5-bin-hadoop2.7\spark-2.4.5-bin-hadoop2.7\bin
	pyspark --jars C:\MyWork\python\gitBusinessRulesEngine\notes\postgresql-42.2.14.jar
	driver = "org.postgresql.Driver"
	
from pyspark.sql import SparkSession

def getSpark():
    ss = SparkSession.builder.appName("Test").getOrCreate()
    return ss

def getUrl(db, user, pwd):
    url = f"jdbc:postgresql://localhost/{db}?user={user}&password={pwd}"
    return url

def getReader(sparkSession,url):
    reader = sparkSession.read.format("jdbc").option("driver","org.postgresql.Driver").option("url", url)
    return reader

def getQueryDataFrame(sparkSession, url, query):
    reader = getReader(sparkSession, url).option("dbtable", f"({query}) T")
    return reader.load()

# -- Basic tests
sparkSession = getSpark()
sparkSession.sparkContext.setLogLevel("ERROR")
# QUERY
query = 'SELECT * FROM Foo'
# URL
url = getUrl('postgres','postgres','*secret*')
# DF
df = getQueryDataFrame(sparkSession, url, query)
df.show()
# Arrays (https://mungingdata.com/apache-spark/arraytype-columns/)
from pyspark.sql.functions import col
from pyspark.sql.functions import explode
df.select(col("name"), explode(col("moremetadata"))).show()
df.select(col("name"), explode(col("metadata"))).show()
# Len of array
from pyspark.sql.functions import size
df.select(col("metadata"),size(col("metadata"))).show()
# element_at
from pyspark.sql.functions import element_at
df.select(col("metadata"),element_at(col("metadata"),3)).show() # returns NULL if no element_at index
# concat
from pyspark.sql.functions import concat
df.select(col("metadata"), col("moremetadata"),concat(col("metadata"),col("moremetadata"))).show()
l = [col("metadata"), col("moremetadata")]
df.select(col("metadata"), col("moremetadata"),concat(*l)).show()
# JSON (https://stackoverflow.com/questions/41107835/pyspark-parse-a-column-of-json-strings)
from pyspark.sql.functions import lit,schema_of_json,from_json
json_schema = spark.read.json(df.rdd.map(lambda row: row.js)).schema
df.withColumn('js', from_json(col('js'), json_schema)).show()

sparkSession.stop()

# - Investigate NULL vs '-' and NULL vs 1900-01-01 comparison
# SQL Code
DROP TABLE IF EXISTS tslp;
DROP TABLE IF EXISTS tetl;
CREATE TABLE tslp
(
	c_str VARCHAR(5),
	c_dt  date,
	c_ts  timestamp,
	c_num int,
	c_f float
);
CREATE TABLE tetl
(
	c_str VARCHAR(5) DEFAULT '-',
	c_dt  date DEFAULT('1900-01-01'),
	c_ts  timestamp DEFAULT ('1900-01-01 00:00:00'),
	c_num int DEFAULT 0,
	c_f float DEFAULT 0.0
);

INSERT INTO tslp (c_dt, c_ts, c_num, c_f)  VALUES ('2020-03-31', '2020-03-31 00:00:00', 1, 1.0);
INSERT INTO tslp (c_str, c_ts, c_num,c_f) VALUES ('val', '2020-03-31 00:00:00', 1, 1.0);
INSERT INTO tslp (c_str, c_dt, c_num,c_f) VALUES ('val', '2020-03-31', 1, 1.0);
INSERT INTO tslp (c_str, c_dt, c_ts, c_f ) VALUES ('val', '2020-03-31', '2020-03-31 00:00:00', 1.0);
INSERT INTO tslp (c_str, c_dt, c_ts, c_num ) VALUES ('val', '2020-03-31', '2020-03-31 00:00:00', 1);

INSERT INTO tetl (c_dt, c_ts, c_num, c_f)  VALUES ('2020-03-31', '2020-03-31 00:00:00', 1, 1.0);
INSERT INTO tetl (c_str, c_ts, c_num,c_f) VALUES ('val', '2020-03-31 00:00:00', 1, 1.0);
INSERT INTO tetl (c_str, c_dt, c_num,c_f) VALUES ('val', '2020-03-31', 1, 1.0);
INSERT INTO tetl (c_str, c_dt, c_ts, c_f ) VALUES ('val', '2020-03-31', '2020-03-31 00:00:00', 1.0);
INSERT INTO tetl (c_str, c_dt, c_ts, c_num ) VALUES ('val', '2020-03-31', '2020-03-31 00:00:00', 1);

SELECT * FROM tslp;
SELECT * FROM tetl;
# - Spark code
sparkSession = getSpark()
sparkSession.sparkContext.setLogLevel("ERROR")
# URL
url = getUrl('postgres','postgres','foobar_secret')

q1 = "SELECT * FROM tslp"
q2 = "SELECT * FROM tetl"
df1 = getQueryDataFrame(sparkSession, url, q1)
df2 = getQueryDataFrame(sparkSession, url, q2)

#defaults map
nullDefaultMap={'c_str':'-', 'c_num' : 0, 'c_f' : 0.0}
df1.na.fill(nullDefaultMap).show()
	
	